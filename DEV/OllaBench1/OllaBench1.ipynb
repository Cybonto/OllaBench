{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "dev_only"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook OllaGen1.ipynb to script\n",
      "[NbConvertApp] Writing 16956 bytes to ..\\OllaGen1.py\n"
     ]
    }
   ],
   "source": [
    "#### Dev Only Cell ####\n",
    "# Run this cell to export production code to .py file in the parent folder\n",
    "# dev_only cells will not be written to .py\n",
    "\n",
    "!jupyter nbconvert --to script OllaBench1.ipynb --output-dir='../' --TagRemovePreprocessor.enabled=True --TagRemovePreprocessor.remove_cell_tags='{\"dev_only\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following models were not pulled in Ollama: ['bogus']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "\n",
    "## Import Python Libraries\n",
    "import os\n",
    "import doctest\n",
    "import datetime\n",
    "import random\n",
    "import contextlib\n",
    "\n",
    "import itertools\n",
    "import copy\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import json\n",
    "\n",
    "import ollama\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "## Import local modules\n",
    "#- n/a\n",
    "\n",
    "params_path=\"params.json\"\n",
    "params={}\n",
    "# Read the parameters from the JSON file\n",
    "try:\n",
    "    with open(params_path, 'r') as file:\n",
    "        params = json.load(file)   \n",
    "except FileNotFoundError:\n",
    "    print(f\"The file {params_path} was not found.\")\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error decoding JSON from the file {params_path}.\")\n",
    "# Initialize variables\n",
    "llm_framework = params[\"llm_framework\"]\n",
    "llm_endpoints = params[\"llm_endpoints\"]\n",
    "llm_models = params[\"llm_models\"]\n",
    "llm_leaderboard = params[\"llm_leaderboard\"]\n",
    "tries = params[\"bench_tries\"]\n",
    "QA_TF_inpath = params[\"QA_TF_inpath\"]\n",
    "QA_WCP_inpath = params[\"QA_WCP_inpath\"]\n",
    "QA_WHO_inpath = params[\"QA_WHO_inpath\"] \n",
    "QA_TF_outpath = params[\"QA_TF_outpath\"]\n",
    "QA_WCP_outpath = params[\"QA_WCP_outpath\"]\n",
    "QA_WHO_outpath = params[\"QA_WHO_outpath\"] \n",
    "\n",
    "if llm_framework==\"ollama\": # only support Ollama as the eval target framework at the moment\n",
    "    from ollama import Client\n",
    "    client = Client(host=llm_endpoints)\n",
    "\n",
    "'''\n",
    "if llm_framework==\"openai\":\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(\n",
    "        # This is the default and can be omitted\n",
    "        api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    )\n",
    "if llm_framework==\"llama_index\":\n",
    "    from llama_index.llms import LocalTensorRTLLM\n",
    "    def completion_to_prompt(completion: str) -> str:\n",
    "        \"\"\"\n",
    "        Given a completion, return the prompt using llama2 format.\n",
    "        \"\"\"\n",
    "        return f\"<s> [INST] {completion} [/INST] \"\n",
    "    llm = LocalTensorRTLLM(\n",
    "    model_path=\"./model\",\n",
    "    engine_name=\"llama_float16_tp1_rank0.engine\",\n",
    "    tokenizer_dir=\"meta-llama/Llama-2-13b-chat\",\n",
    "    completion_to_prompt=completion_to_prompt)\n",
    "'''\n",
    "\n",
    "# Prepare the list of targetted LLM models\n",
    "llm_list = [d[next(iter(d))] for d in ollama.list()['models']] #get model names from the list of dict returned by Ollama\n",
    "if llm_models==\"all\":\n",
    "    llm_models=llm_list\n",
    "else:\n",
    "    llm_names_bak=llm_models.copy()\n",
    "    llm_models[:] = [item for item in llm_models if item in llm_list] #remove model names that are not installed\n",
    "    print(\"The following models were not pulled in Ollama: \"+str([item for item in llm_names_bak if item not in llm_models]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "\n",
    "def load_csv_to_dataframe(file_path):\n",
    "    try:\n",
    "        # Load the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "\n",
    "def write_df_to_csv(df, csv_file):\n",
    "    # If the CSV file already exists, append to it, otherwise create a new file\n",
    "    mode = 'a' if os.path.exists(csv_file) else 'w'\n",
    "\n",
    "    # Write the DataFrame to CSV\n",
    "    df.to_csv(csv_file, mode=mode, index=False)\n",
    "    \n",
    "def test_models(tries,llm_models):\n",
    "    \"\"\"\n",
    "    A function to check for bad LLM models.\n",
    "    \n",
    "    Parameters:\n",
    "    tries: the number of failed attempts before reporting\n",
    "    llm_models: a list of targeted LLM models\n",
    "    \n",
    "    Returns:\n",
    "    bad_models : a list of models that fail to load\n",
    "    \"\"\"\n",
    "    bad_models=[]\n",
    "    for a_model in llm_models:\n",
    "        while tries>0:\n",
    "            try:\n",
    "                response = ollama.chat(model=a_model, messages=[{'role': 'user', 'content': 'Just say Yes'}])\n",
    "                tries=0\n",
    "            except Exception as e:\n",
    "                tries-=1\n",
    "                response = str(e) #\"error\" wil be in response\n",
    "        if \"error\" in response:\n",
    "            bad_models.append(a_model)\n",
    "    if len(bad_models)>0:\n",
    "        print(\"The following models are bad: \"+str(bad_models))\n",
    "    return bad_models\n",
    "\n",
    "def check_answer (reference, answer):\n",
    "    \"\"\"\n",
    "    Check if the correct answer (reference) is within the first two sentences of a model's answer.\n",
    "    \n",
    "    Parameters:\n",
    "    reference : the reference answer\n",
    "    answer : the model's answer\n",
    "    \n",
    "    Returns:\n",
    "    True or False\n",
    "    \"\"\"\n",
    "    # Tokenize string2 into sentences\n",
    "    sentences = sent_tokenize(answer)\n",
    "\n",
    "    # Combine the first two sentences into one string\n",
    "    first_two_sentences = ' '.join(sentences[:2])\n",
    "\n",
    "    # Check if string1 is in the combined first two sentences\n",
    "    if reference in first_two_sentences:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def grade_model(a_model,input_df):\n",
    "    \"\"\"\n",
    "    A function to grade an LLM model's responses\n",
    "    \n",
    "    Parameters:\n",
    "    a_model : the target LLM model\n",
    "    input_df : input dataframe that is consistent with OllaGen1 output format\n",
    "    \n",
    "    Returns:\n",
    "    Results_df : a result df with input_df columns and additional columns of\n",
    "    'Model','Total Duration','Eval Counts','Model Response','Score','Notes'\n",
    "    \"\"\"\n",
    "    results=[]\n",
    "    for index, row in input_df.iterrows():\n",
    "        score=0\n",
    "        notes='none'\n",
    "        response = ollama.generate(model='llama2', prompt= str(row[\"Context\"]+row[\"Question\"]), stream=False)\n",
    "        if check_answer(str(row['Answer']).lower(),str(response['response']).lower()):\n",
    "            # give score if reference answer is within the first two sentences of model's answer\n",
    "            score = 1\n",
    "        elif str(row['Answer']).lower() in str(response['response']).lower():\n",
    "            # if correct answer exist somewhere else, flag for human verification\n",
    "            notes = \"Needs verification\"\n",
    "        results.append([row['ID'], row['Context'], row['Question'], row['Answer'],\n",
    "                        response['model'],response['total_duration'],response['eval_count'],\n",
    "                        response['response'],score, notes\n",
    "                        ])\n",
    "    results_df = pd.DataFrame(results,columns=['ID', 'Context', 'Question', 'Reference Answer',\n",
    "                                                    'Model','Total Duration','Eval Counts','Model Response',\n",
    "                                                    'Score','Notes'\n",
    "                                                    ])\n",
    "    return results_df\n",
    "    \n",
    "def function_template ():\n",
    "    \"\"\"\n",
    "    Function description.\n",
    "    \n",
    "    Parameters:\n",
    "    param : param description\n",
    "    \n",
    "    Returns:\n",
    "    value : value description\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return\n",
    "        # function code here\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MAIN\n",
    "\n",
    "# Load QA datasets\n",
    "QA_TF_df = load_csv_to_dataframe(QA_TF_inpath)\n",
    "QA_WCP_df = load_csv_to_dataframe(QA_WCP_inpath)\n",
    "QA_WHO_df = load_csv_to_dataframe(QA_WHO_inpath)\n",
    "\n",
    "# Test load all targeted models first and remove models that failed to load\n",
    "bad_models = test_models(tries,llm_models)\n",
    "llm_models = [item for item in llm_models if item not in bad_models]\n",
    "\n",
    "# Get Results\n",
    "for a_model in llm_models:\n",
    "    if len(QA_TF_df)>0:\n",
    "        QA_TF_result_df = grade_model(a_model,QA_TF_df)\n",
    "        write_df_to_csv(QA_TF_result_df,QA_TF_outpath)\n",
    "    if len(QA_WCP_df)>0:\n",
    "        QA_WCP_result_df = grade_model(a_model,QA_WCP_df)\n",
    "        write_df_to_csv(QA_WCP_result_df,QA_WCP_outpath)\n",
    "    if len(QA_WHO_df)>0:\n",
    "        QA_WHO_result_df = grade_model(a_model,QA_WHO_df)\n",
    "        write_df_to_csv(QA_WHO_result_df,QA_WHO_outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": [
     "dev_only"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor a_model in llm_models:\\n    if len(QA_TF_result_df)>0:\\n        for item in QA_TF_result_df:\\n            #print(\" \")\\n    if len(QA_WCP_result_df)>0:\\n        for item in QA_WCP_result_df:\\n            #print(\" \")\\n    if len(QA_WHO_result_df)>0:\\n        for item in QA_WHO_result_df:\\n            #print(\" \")\\n'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Dev Only Cell ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "dev_only"
    ]
   },
   "outputs": [],
   "source": [
    "#### Dev Only Cell ####\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
