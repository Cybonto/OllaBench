{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "dev_only"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook OllaBench1_v.0.2.ipynb to script\n",
      "[NbConvertApp] Writing 11455 bytes to ..\\..\\OllaBench1.py\n"
     ]
    }
   ],
   "source": [
    "#### Dev Only Cell ####\n",
    "# Run this cell to export production code to .py file in the parent folder\n",
    "# Make sure to save the notebook first\n",
    "# dev_only cells will not be written to .py\n",
    "import shutil\n",
    "shutil.copyfile('./params.json', '../../params.json')\n",
    "!jupyter nbconvert --to script OllaBench1_v.0.2.ipynb --output OllaBench1 --output-dir='../../' --TagRemovePreprocessor.enabled=True --TagRemovePreprocessor.remove_cell_tags='{\"dev_only\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following model(s) does not exist in Ollama: ['bogus']\n"
     ]
    }
   ],
   "source": [
    "# OllaBench1 v.0.2\n",
    "# IMPORTS\n",
    "\n",
    "## Import Python Libraries\n",
    "import os\n",
    "import doctest\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import contextlib\n",
    "\n",
    "import itertools\n",
    "import copy\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import json\n",
    "\n",
    "import ollama\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "params_path=\"params.json\"\n",
    "params={}\n",
    "# Read the parameters from the JSON file\n",
    "try:\n",
    "    with open(params_path, 'r') as file:\n",
    "        params = json.load(file)   \n",
    "except FileNotFoundError:\n",
    "    print(f\"The file {params_path} was not found.\")\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error decoding JSON from the file {params_path}.\")\n",
    "# Initialize variables\n",
    "llm_framework = params[\"llm_framework\"]\n",
    "llm_endpoints = params[\"llm_endpoints\"]\n",
    "llm_models = params[\"llm_models\"]\n",
    "llm_leaderboard = params[\"llm_leaderboard\"]\n",
    "tries = params[\"bench_tries\"]\n",
    "QA_inpath = params[\"QA_inpath\"] \n",
    "\n",
    "if llm_framework==\"ollama\": # only support Ollama as the eval target framework at the moment\n",
    "    from ollama import Client\n",
    "    client = Client(host=llm_endpoints)\n",
    "\n",
    "'''\n",
    "if llm_framework==\"openai\":\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(\n",
    "        # This is the default and can be omitted\n",
    "        api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    )\n",
    "if llm_framework==\"llama_index\":\n",
    "    from llama_index.llms import LocalTensorRTLLM\n",
    "    def completion_to_prompt(completion: str) -> str:\n",
    "        \"\"\"\n",
    "        Given a completion, return the prompt using llama2 format.\n",
    "        \"\"\"\n",
    "        return f\"<s> [INST] {completion} [/INST] \"\n",
    "    llm = LocalTensorRTLLM(\n",
    "    model_path=\"./model\",\n",
    "    engine_name=\"llama_float16_tp1_rank0.engine\",\n",
    "    tokenizer_dir=\"meta-llama/Llama-2-13b-chat\",\n",
    "    completion_to_prompt=completion_to_prompt)\n",
    "'''\n",
    "\n",
    "# Prepare the list of targetted LLM models\n",
    "llm_list = [d[next(iter(d))] for d in ollama.list()['models']] #get model names from the list of dict returned by Ollama\n",
    "if llm_models==\"all\":\n",
    "    llm_models=llm_list\n",
    "else:\n",
    "    llm_names_bak=llm_models.copy()\n",
    "    llm_models[:] = [item for item in llm_models if item in llm_list] #remove model names that are not installed\n",
    "    print(\"The following model(s) does not exist in Ollama: \"+str([item for item in llm_names_bak if item not in llm_models]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "\n",
    "\n",
    "\n",
    "def write_df_to_csv(df, csv_file):\n",
    "    # If the CSV file already exists, append to it, otherwise create a new file\n",
    "    mode = 'a' if os.path.exists(csv_file) else 'w'\n",
    "\n",
    "    # Write the DataFrame to CSV\n",
    "    df.to_csv(csv_file, mode=mode, index=False)\n",
    "    \n",
    "def test_model(tries,a_model):\n",
    "    \"\"\"\n",
    "    A function to check for bad LLM models.\n",
    "    \n",
    "    Parameters:\n",
    "    tries: the number of failed attempts before reporting\n",
    "    llm_models: a list of targeted LLM models\n",
    "    \n",
    "    Returns:\n",
    "    True if the model is good\n",
    "    \"\"\"\n",
    "    print(f\"Test loading of {a_model}\")\n",
    "    while tries>0:\n",
    "        try:\n",
    "            response = get_response(llm_framework,a_model,'just say yes')\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            tries-=1\n",
    "            response = str(e) #\"error\" wil be in response\n",
    "    if \"error\" in response:\n",
    "        print(f\"The model {a_model} is bad.\")\n",
    "    return False \n",
    "\n",
    "def check_answer (reference, answer):\n",
    "    \"\"\"\n",
    "    Check if the correct answer (reference) is within the first two sentences of a model's answer.\n",
    "    \n",
    "    Parameters:\n",
    "    reference : the reference answer\n",
    "    answer : the model's answer\n",
    "    \n",
    "    Returns:\n",
    "    True or False\n",
    "    \"\"\"\n",
    "    norm_ref1 = str(reference).lower()\n",
    "    norm_ref2 = norm_ref1.split(\" - \")[0]\n",
    "    norm_ref3 = norm_ref1.split(\" - \")[1]\n",
    "    norm_ref4 = norm_ref2.replace(\"option \",\"\")\n",
    "    ans_keys = (norm_ref1, norm_ref2, norm_ref3, norm_ref4)\n",
    "    norm_answer = str(answer).lower()\n",
    "\n",
    "    # Tokenize string2 into sentences\n",
    "    sentences = sent_tokenize(norm_answer)\n",
    "\n",
    "    # Combine the first two sentences into one string\n",
    "    first_two_sentences = ' '.join(sentences[:2])\n",
    "\n",
    "    # Check if string1 is in the combined first two sentences\n",
    "    if any(ans_key in first_two_sentences for ans_key in ans_keys):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_response(llm_framework,a_model,a_prompt):\n",
    "    cache=\"\"\n",
    "    if llm_framework ==\"ollama\":\n",
    "        result = ollama.generate(model=a_model, prompt= a_prompt, stream=False)\n",
    "        while cache != str(result['response']):\n",
    "            cache = str(result['response'])\n",
    "            time.sleep(1)\n",
    "    return result\n",
    "\n",
    "def grade_model(a_model,input_df):\n",
    "    \"\"\"\n",
    "    A function to grade an LLM model's responses\n",
    "    \n",
    "    Parameters:\n",
    "    a_model : the target LLM model\n",
    "    input_df : input dataframe that is consistent with OllaGen1 output format\n",
    "    \n",
    "    Returns:\n",
    "    Results_df : a result df with input_df columns and additional columns of\n",
    "    'Model','Total Duration','Eval Counts','Model Response','Score','Notes'\n",
    "    \"\"\"\n",
    "    results=[]\n",
    "\n",
    "    print(f\"Grading {a_model}\")\n",
    "    # load the model in Ollama\n",
    "    get_response(\"ollama\",a_model,'just say yes')\n",
    "\n",
    "    greenlight = True\n",
    "    for index, row in input_df.iterrows():\n",
    "        while not greenlight:\n",
    "            time.sleep(10)\n",
    "        greenlight = False\n",
    "\n",
    "        score=0\n",
    "\n",
    "        context = f\"\"\"\n",
    "            Here are the intelligence about {row[\"P1_name\"]} with comments from trusted experts and/or {row[\"P1_name\"]}'s recorded statement(s).\n",
    "            {row[\"P1_profile\"]}\n",
    "            Here are the intelligence about {row[\"P2_name\"]} with comments from trusted experts and/or {row[\"P2_name\"]}'s recorded statement(s).\n",
    "            {row[\"P2_profile\"]}\n",
    "\n",
    "            \"\"\"\n",
    "        #print(context)\n",
    "        WCP_Question = row[\"WCP_Question\"]\n",
    "        WCP_Answer = row[\"WCP_Answer\"]\n",
    "        WCP_score = 0\n",
    "        WHO_Question = row[\"WHO_Question\"]\n",
    "        WHO_Answer = row[\"WHO_Answer\"]\n",
    "        WHO_score = 0\n",
    "        TeamRisk_Question = row[\"TeamRisk_Question\"]\n",
    "        TeamRisk_Answer = row[\"TeamRisk_Answer\"]\n",
    "        TeamRisk_score = 0\n",
    "        TargetFactor_Question = row[\"TargetFactor_Question\"]\n",
    "        TargetFactor_Answer = row[\"TargetFactor_Answer\"]\n",
    "        TargetFactor_score = 0\n",
    "\n",
    "        flag=\"init\"\n",
    "        WCP_response = get_response(\"ollama\",a_model,str(context+WCP_Question))\n",
    "        while flag!= WCP_response['response']:\n",
    "            time.sleep(1)\n",
    "            flag= WCP_response['response']\n",
    "        if check_answer(WCP_Answer,WCP_response['response']):\n",
    "            WCP_score = 1\n",
    "        \n",
    "        WHO_response = get_response(\"ollama\",a_model,str(context+WHO_Question))\n",
    "        while flag!= WHO_response['response']:\n",
    "            time.sleep(1)\n",
    "            flag= WHO_response['response']\n",
    "        if check_answer(WHO_Answer,WHO_response['response']):\n",
    "            WHO_score = 1\n",
    "\n",
    "        TeamRisk_response = get_response(\"ollama\",a_model,str(context+TeamRisk_Question))\n",
    "        while flag!= TeamRisk_response['response']:\n",
    "            time.sleep(1)\n",
    "            flag= TeamRisk_response['response']\n",
    "        if check_answer(TeamRisk_Answer,TeamRisk_response['response']):\n",
    "            TeamRisk_score = 1\n",
    "        \n",
    "        TargetFactor_response = get_response(\"ollama\",a_model,str(context+TargetFactor_Question))\n",
    "        while flag!= TargetFactor_response['response']:\n",
    "            time.sleep(1)\n",
    "            flag= TargetFactor_response['response']\n",
    "        if check_answer(TargetFactor_Answer,TargetFactor_response['response']):\n",
    "            TargetFactor_score = 1\n",
    "\n",
    "        score = WCP_score+WHO_score+TeamRisk_score+TargetFactor_score\n",
    "\n",
    "        results.append([row['ID'], a_model, str(context), WCP_Question, WCP_Answer,\n",
    "                        WCP_response['total_duration'],WCP_response['eval_count'], str(WCP_response['response']),WCP_score,\n",
    "                        WHO_Question, WHO_Answer,\n",
    "                        WHO_response['total_duration'],WHO_response['eval_count'], str(WHO_response['response']),WHO_score,\n",
    "                        TeamRisk_Question, TeamRisk_Answer,\n",
    "                        TeamRisk_response['total_duration'],TeamRisk_response['eval_count'], str(TeamRisk_response['response']),TeamRisk_score,\n",
    "                        TargetFactor_Question, TargetFactor_Answer,\n",
    "                        TargetFactor_response['total_duration'],TargetFactor_response['eval_count'], str(TargetFactor_response['response']),TargetFactor_score,\n",
    "                        score\n",
    "                        ])\n",
    "        if index%50==0:\n",
    "            print(\".\", end =\" \", flush=True)\n",
    "\n",
    "        greenlight = True\n",
    "        \n",
    "    results_df = pd.DataFrame(results,columns=['ID', 'Model', 'Context', 'WCP_Question', 'WCP_Correct_Answer',\n",
    "                                                    'WCP_TotalDuration','WCP_EvalCounts','WCP_Response','WCP_score',\n",
    "                                                    'WHO_Question', 'WHO_Correct_Answer',\n",
    "                                                    'WHO_TotalDuration','WHO_EvalCounts','WHO_Response','WHO_score',\n",
    "                                                    'TeamRisk_Question', 'TeamRisk_Correct_Answer',\n",
    "                                                    'TeamRisk_TotalDuration','TeamRisk_EvalCounts','TeamRisk_Response','TeamRisk_score',\n",
    "                                                    'TargetFactor_Question', 'TargetFactor_Correct_Answer',\n",
    "                                                    'TargetFactor_TotalDuration','TargetFactor_EvalCounts','TargetFactor_Response','TargetFactor_score',\n",
    "                                                    'Total score'\n",
    "                                                    ])\n",
    "    print(\" \")\n",
    "    return results_df\n",
    "\n",
    "def df_2_chunks (a_df,chunk_size):\n",
    "    \"\"\"\n",
    "    Function description.\n",
    "    \n",
    "    Parameters:\n",
    "    param : param description\n",
    "    \n",
    "    Returns:\n",
    "    value : value description\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Calculate the total number of chunks to be created\n",
    "        num_chunks = len(a_df) // chunk_size + (1 if len(a_df) % chunk_size else 0)\n",
    "        chunks=[]\n",
    "        for i in range(num_chunks):\n",
    "            # Slice the DataFrame to create a chunk\n",
    "            start_row = i * chunk_size\n",
    "            end_row = start_row + chunk_size\n",
    "            chunk = a_df[start_row:end_row]\n",
    "    \n",
    "            # Append the chunk to the list of chunks\n",
    "            chunks.append(chunk)\n",
    "        return chunks\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "        \n",
    "def function_template ():\n",
    "    \"\"\"\n",
    "    Function description.\n",
    "    \n",
    "    Parameters:\n",
    "    param : param description\n",
    "    \n",
    "    Returns:\n",
    "    value : value description\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return\n",
    "        # function code here\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loading of llama2:7b\n",
      "Grading llama2:7b\n",
      ".  \n",
      "Graded results were saved to llama2-7b_2024-04-02_14-41_QA_Results.csv\n",
      "Test loading of orca2:7b\n"
     ]
    }
   ],
   "source": [
    "## MAIN\n",
    "\n",
    "# Load QA datasets\n",
    "QA_df = pd.read_csv(QA_inpath,header=0)\n",
    "# Split to chunks\n",
    "chunk_size = 500\n",
    "QA_df_chunks = df_2_chunks(QA_df,chunk_size)\n",
    "greenlight = True\n",
    "greenlight2 = True\n",
    "\n",
    "# Get Results\n",
    "for a_model in llm_models:\n",
    "    while not greenlight:\n",
    "        time.sleep(10)\n",
    "    greenlight=False # turn off greenlight here, pay attn to when it will be turned on again!\n",
    "    test_passed = test_model(tries,a_model)\n",
    "    if test_passed:\n",
    "        for index,QA_df_chunk in enumerate(QA_df_chunks):\n",
    "            while not greenlight2:\n",
    "                time.sleep(60)\n",
    "            greenlight2=False # turn off greenlight2 here, pay attn to when it will be turned on again!\n",
    "            print(f\"Load chunk {index}\")\n",
    "            QA_outpath = a_model.replace(\":\",\"-\")+\"_chunk\"+str(index)+\"_\"+datetime.now().strftime(\"%Y-%m-%d_%H-%M\")+\"_QA_Results.csv\"\n",
    "            if len(QA_df)>0:\n",
    "                QA_result_df = grade_model(a_model,QA_df_chunk)\n",
    "                if len(QA_result_df)>0:\n",
    "                    write_df_to_csv(QA_result_df,QA_outpath)\n",
    "                    print(f\"Graded results were saved to {QA_outpath}\")\n",
    "                    greenlight2 = True\n",
    "                else:\n",
    "                    print(f\"Result is empty. Nothing was saved to {QA_outpath}\")\n",
    "                    greenlight2 = True\n",
    "            else:\n",
    "                print(\"Input dataframe is empty. Program exiting.\")\n",
    "                greenlight2 = True\n",
    "        greenlight=True\n",
    "    else:\n",
    "        print(f\"There are issues with loading {a_model}. Skip the grading of this model.\")\n",
    "        print(\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "dev_only"
    ]
   },
   "outputs": [],
   "source": [
    "#### Dev Only Cell ####\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
